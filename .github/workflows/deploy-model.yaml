name: Deploy Model to k3s

# Reusable workflow for deploying the model to k3s cluster
# Used by both training workflow (after model improvement) and release workflow (code changes)

on:
  workflow_call:
    inputs:
      trigger_reason:
        description: 'Deployment trigger: training, release, or manual'
        type: string
        required: false
        default: 'manual'
    secrets:
      AWS_ACCESS_KEY_ID:
        required: false
      AWS_SECRET_ACCESS_KEY:
        required: false
      AWS_SESSION_TOKEN:
        required: false
  workflow_dispatch:
    inputs:
      trigger_reason:
        description: 'Deployment trigger reason'
        type: string
        required: false
        default: 'manual-test'

jobs:
  deploy:
    name: Build & Deploy to k3s
    runs-on: self-hosted  # CRITICAL: Must run on EC2 for k3s access
    env:
      KUBECONFIG: /home/ec2-user/.kube/config  # Use ec2-user kubeconfig instead of root's k3s.yaml
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      # No need to configure AWS credentials - EC2 instance has IAM role (LabInstanceProfile)
      # Self-hosted runner inherits EC2's instance profile credentials automatically

      - name: Login to ECR
        run: |
          python3 << 'EOF'
          import boto3
          import subprocess
          import base64

          ecr = boto3.client('ecr', region_name='us-east-1')
          token = ecr.get_authorization_token()
          auth_token = token['authorizationData'][0]['authorizationToken']
          username, password = base64.b64decode(auth_token).decode().split(':')

          subprocess.run([
              'docker', 'login',
              '--username', username,
              '--password-stdin',
              '055677744286.dkr.ecr.us-east-1.amazonaws.com'
          ], input=password.encode(), check=True)
          EOF

      - name: Build + Push Docker
        run: |
          ECR_REPO=055677744286.dkr.ecr.us-east-1.amazonaws.com/wms-model
          docker build -f docker/Dockerfile.serve -t $ECR_REPO:latest .
          docker push $ECR_REPO:latest

      - name: Create ECR ImagePullSecret
        run: |
          # Delete old secret if exists (to update credentials)
          kubectl delete secret ecr-secret -n default --ignore-not-found=true

          # Get ECR authorization token using boto3 (same as Docker login step)
          ECR_TOKEN=$(python3 << 'EOF'
          import boto3
          import base64

          ecr = boto3.client('ecr', region_name='us-east-1')
          token = ecr.get_authorization_token()
          auth_token = token['authorizationData'][0]['authorizationToken']
          username, password = base64.b64decode(auth_token).decode().split(':')
          print(password)
          EOF
          )

          # Create new secret with fresh credentials
          kubectl create secret docker-registry ecr-secret \
            --docker-server=055677744286.dkr.ecr.us-east-1.amazonaws.com \
            --docker-username=AWS \
            --docker-password="$ECR_TOKEN" \
            --namespace=default

          echo "âœ… ECR imagePullSecret created/updated in default namespace"

          # Force restart of any existing pods to pick up new secret
          kubectl delete pods -l app=ml-model,release=wms-model -n default --ignore-not-found=true
          echo "ðŸ”„ Deleted old pods to force restart with new credentials"

      - name: Validate Production Model Exists
        env:
          MLFLOW_TRACKING_URI: http://localhost:5000
        run: |
          python3 << 'EOF'
          import os
          import sys
          import mlflow
          from mlflow.tracking import MlflowClient

          mlflow_uri = os.environ.get("MLFLOW_TRACKING_URI", "http://localhost:5000")
          mlflow.set_tracking_uri(mlflow_uri)
          client = MlflowClient()
          model_name = "water-meter-segmentation"

          try:
              # First, check if model is registered at all
              try:
                  registered_model = client.get_registered_model(model_name)
              except mlflow.exceptions.RestException as e:
                  if "RESOURCE_DOES_NOT_EXIST" in str(e):
                      print("âŒ FATAL: Model not registered in MLflow")
                      print(f"   Model name: {model_name}")
                      print(f"   MLflow URI: {mlflow_uri}")
                      print("")
                      print("Cannot deploy - train a model first:")
                      print("  1. Push training data to trigger train.yml workflow")
                      print("  2. Wait for training to complete")
                      print("  3. Verify model registered in MLflow")
                      print("  4. Then retry deployment")
                      sys.exit(1)
                  else:
                      raise

              # Model exists, now check for Production stage
              # Note: get_latest_versions is deprecated but still works
              versions = client.get_latest_versions(model_name, stages=["Production"])

              if not versions:
                  print("âŒ FATAL: No Production model found in MLflow")
                  print(f"   Model name: {model_name}")
                  print(f"   Registered versions: {len(registered_model.latest_versions)}")
                  print(f"   MLflow URI: {mlflow_uri}")
                  print("")
                  print("Model is registered but no version in Production stage.")
                  print("Train and promote a model first.")
                  sys.exit(1)

              model = versions[0]
              print(f"âœ… Found Production model: {model_name} v{model.version}")
              print(f"   Run ID: {model.run_id}")
              print(f"   Source: {model.source}")

          except Exception as e:
              print(f"âŒ MLflow connection error: {e}")
              print(f"   URI: {mlflow_uri}")
              print("   Ensure EC2 is running and MLflow is accessible")
              sys.exit(1)
          EOF

      - name: Deploy via Helm
        run: |
          helm upgrade --install wms-model devops/helm/ml-model/ \
            -f infrastructure/helm-values.yaml \
            --namespace default \
            --create-namespace

      - name: Debug - Check Deployment State
        if: always()
        run: |
          echo "=== Helm Releases ==="
          helm list -A

          echo -e "\n=== All Deployments ==="
          kubectl get deployments -A

          echo -e "\n=== All Pods ==="
          kubectl get pods -A --show-labels

          echo -e "\n=== Pods in default namespace ==="
          kubectl get pods -n default -o wide

          echo -e "\n=== Describe wms-model deployment (if exists) ==="
          kubectl describe deployment -l release=wms-model -n default || echo "No deployment found"

          echo -e "\n=== Events ==="
          kubectl get events -n default --sort-by='.lastTimestamp' | tail -20

      - name: Verify Deployment
        run: |
          echo "Waiting for pod to be ready (up to 10 min, image pull ~8GB)..."
          kubectl wait --for=condition=ready pod \
            -l app=ml-model,release=wms-model \
            --timeout=600s || {
            echo "âŒ Pod not ready in time. Logs:"
            kubectl logs -l app=ml-model,release=wms-model --tail=50 || true
            echo "Pod status:"
            kubectl get pod -l app=ml-model,release=wms-model -o wide
            kubectl describe pod -l app=ml-model,release=wms-model | tail -30
            exit 1
          }

          echo "Testing health endpoint..."
          POD=$(kubectl get pod -l app=ml-model,release=wms-model -o jsonpath='{.items[0].metadata.name}')
          kubectl exec "$POD" -- python3 -c "import urllib.request; r = urllib.request.urlopen('http://localhost:8000/health'); print(r.read().decode())"

          echo "âœ… Deployment verified successfully"

      - name: Cleanup Old Deployments
        if: success()
        run: |
          echo "Cleaning up old model-* namespaces..."

          # Find all old model-* namespaces (exclude default)
          OLD_NAMESPACES=$(kubectl get namespaces -o json | \
            jq -r '.items[].metadata.name | select(startswith("model-"))')

          if [[ -z "$OLD_NAMESPACES" ]]; then
            echo "âœ… No old namespaces to clean up"
            exit 0
          fi

          echo "Found old namespaces:"
          echo "$OLD_NAMESPACES" | while read -r ns; do echo "  - $ns"; done

          # Uninstall Helm releases first
          echo "$OLD_NAMESPACES" | while read -r ns; do
            RELEASES=$(helm list -n "$ns" -q 2>/dev/null || true)
            if [[ -n "$RELEASES" ]]; then
              echo "$RELEASES" | while read -r release; do
                echo "Uninstalling: $release (namespace: $ns)"
                helm uninstall "$release" -n "$ns" || true
              done
            fi
          done

          # Delete namespaces
          echo "$OLD_NAMESPACES" | while read -r ns; do
            echo "Deleting namespace: $ns"
            kubectl delete namespace "$ns" --timeout=60s || true
          done

          echo "âœ… Cleanup complete!"

      - name: Deployment Summary
        if: always()
        run: |
          echo "## Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger:** ${{ inputs.trigger_reason }}" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ job.status }}" = "success" ]; then
            echo "âœ… Model deployed successfully to k3s cluster" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Deployment failed - check logs above" >> $GITHUB_STEP_SUMMARY
          fi
