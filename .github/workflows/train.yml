name: Train Model

on:
  pull_request:
    branches:
      - main
    paths:
      - 'WMS/data/training/**'  # Any training data changes
      - 'WMS/src/**'
      - 'WMS/configs/**'
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: write
  issues: write
  pull-requests: write

# Prevent concurrent training workflows
concurrency:
  group: training-${{ github.ref }}
  cancel-in-progress: false

jobs:
  # Data Quality Assurance - validate before training
  data-qa:
    name: Data Quality Assurance
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: pip install opencv-python numpy Pillow PyYAML

      - name: Run Data QA
        run: |
          python devops/scripts/data-qa.py WMS/data/training/ --output report.json

          # Display report
          cat report.json

          # Check status
          if grep -q '"status": "FAIL"' report.json; then
            echo "‚ùå Data QA failed!"
            exit 1
          fi

          echo "‚úÖ Data QA passed!"

      - name: Post QA report as PR comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = JSON.parse(fs.readFileSync('report.json', 'utf8'));
            const status = report.status === 'PASS' ? '‚úÖ PASSED' : '‚ùå FAILED';

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number,
              body: `## Data Quality Assurance ${status}\n\n\`\`\`json\n${JSON.stringify(report, null, 2)}\n\`\`\``
            });

  # Start EC2 infrastructure
  start-infra:
    name: Start EC2 Infrastructure
    needs: data-qa
    uses: ./.github/workflows/ec2-control.yaml
    with:
      action: start
    secrets: inherit

  # Single training run (time-optimized)
  train:
    name: Train Model
    needs: start-infra
    runs-on: ubuntu-latest
    outputs:
      improved: ${{ steps.quality_gate.outputs.improved }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install dvc[s3]

      - name: Pull training data with DVC (if needed)
        run: |
          cd WMS/data/training

          # Check if data already in repo (from data branch)
          if [ -d "images" ] && [ "$(ls -A images)" ]; then
            echo "‚úÖ Training data already in repository"
          elif [ -f "images.dvc" ]; then
            echo "üì• Pulling data from S3 with DVC..."
            dvc pull images.dvc masks.dvc || echo "‚ö†Ô∏è  DVC pull failed, using local data"
          else
            echo "‚ö†Ô∏è  No training data found"
            exit 1
          fi

      - name: Wait for MLflow server
        run: |
          MLFLOW_URL="${{ needs.start-infra.outputs.mlflow_url }}"
          echo "‚è≥ Waiting for MLflow server at $MLFLOW_URL"

          MAX_ATTEMPTS=30
          ATTEMPT=0

          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if curl -sf "$MLFLOW_URL/health" > /dev/null 2>&1; then
              echo "‚úÖ MLflow server is ready!"
              exit 0
            fi

            ATTEMPT=$((ATTEMPT + 1))
            echo "Attempt $ATTEMPT/$MAX_ATTEMPTS - waiting 10s..."
            sleep 10
          done

          echo "‚ùå MLflow server did not become ready (5 minutes)"
          exit 1

      - name: Train model
        env:
          MLFLOW_TRACKING_URI: ${{ needs.start-infra.outputs.mlflow_url }}
          MLFLOW_HTTP_REQUEST_TIMEOUT: 300
          MLFLOW_HTTP_REQUEST_MAX_RETRIES: 5
          PYTHONPATH: ${{ github.workspace }}
        run: |
          echo "=== Training Model ==="
          echo "MLflow URL: $MLFLOW_TRACKING_URI"
          cd WMS/src
          python train.py --config ../configs/train.yaml --seed ${{ github.run_number }}

      - name: Quality Gate - Compare with Production baseline
        id: quality_gate
        env:
          MLFLOW_TRACKING_URI: ${{ needs.start-infra.outputs.mlflow_url }}
          MLFLOW_HTTP_REQUEST_TIMEOUT: 300
          MLFLOW_HTTP_REQUEST_MAX_RETRIES: 5
        run: |
          python3 << 'EOF'
          import mlflow
          import json
          import os

          mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
          client = mlflow.tracking.MlflowClient()

          # Get latest run from experiment
          experiment = client.get_experiment_by_name("water-meter-segmentation")
          if not experiment:
              print("‚ùå Experiment not found")
              exit(1)

          runs = client.search_runs(
              experiment_ids=[experiment.experiment_id],
              order_by=["start_time DESC"],
              max_results=1
          )

          if not runs:
              print("‚ùå No runs found")
              exit(1)

          run = runs[0]
          metrics = run.data.metrics

          # Get current model metrics
          current_dice = (
              metrics.get('final_test_dice') or
              metrics.get('test_dice') or
              metrics.get('val_dice', 0)
          )
          current_iou = (
              metrics.get('final_test_iou') or
              metrics.get('test_iou') or
              metrics.get('val_iou', 0)
          )

          # Get baseline from Production model (if exists)
          try:
              prod_versions = client.get_latest_versions("water-meter-segmentation", stages=["Production"])
              if prod_versions:
                  prod_version = prod_versions[0]
                  prod_run = client.get_run(prod_version.run_id)

                  prod_metrics = prod_run.data.metrics
                  baseline_dice = (
                      prod_metrics.get('final_test_dice') or
                      prod_metrics.get('test_dice') or
                      prod_metrics.get('val_dice', 0)
                  )
                  baseline_iou = (
                      prod_metrics.get('final_test_iou') or
                      prod_metrics.get('test_iou') or
                      prod_metrics.get('val_iou', 0)
                  )
                  print(f"üìä Production baseline: Dice={baseline_dice:.4f}, IoU={baseline_iou:.4f}")
              else:
                  baseline_dice = 0.0
                  baseline_iou = 0.0
                  print("üìä No Production model - first training (baseline=0)")
          except Exception as e:
              print(f"‚ö†Ô∏è  Could not fetch Production baseline: {e}")
              baseline_dice = 0.0
              baseline_iou = 0.0

          # Quality gate: model must improve (no threshold, pure comparison)
          improved = current_dice > baseline_dice and current_iou > baseline_iou

          result = {
              "run_id": run.info.run_id,
              "metrics": {
                  "dice": current_dice,
                  "iou": current_iou
              },
              "baseline": {
                  "dice": baseline_dice,
                  "iou": baseline_iou
              },
              "improved": improved
          }

          print("\n" + "="*50)
          print("QUALITY GATE RESULTS")
          print("="*50)
          print(json.dumps(result, indent=2))
          print("="*50)

          # Save results
          with open('training_results.json', 'w') as f:
              json.dump(result, f, indent=2)

          # Output for GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"improved={str(improved).lower()}\n")
              f.write(f"dice={current_dice}\n")
              f.write(f"iou={current_iou}\n")
              f.write(f"run_id={run.info.run_id}\n")
              f.write(f"baseline_dice={baseline_dice}\n")
              f.write(f"baseline_iou={baseline_iou}\n")
          EOF

      - name: Upload results artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: training-results
          path: training_results.json
          retention-days: 7

      # If improved: promote model and deploy
      - name: Promote model to Production
        if: steps.quality_gate.outputs.improved == 'true'
        env:
          MLFLOW_TRACKING_URI: ${{ needs.start-infra.outputs.mlflow_url }}
          MLFLOW_HTTP_REQUEST_TIMEOUT: 300
          MLFLOW_HTTP_REQUEST_MAX_RETRIES: 5
        run: |
          python3 << 'EOF'
          import mlflow
          from mlflow.tracking import MlflowClient
          import json
          import os

          mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
          client = MlflowClient()

          # Load results
          with open('training_results.json', 'r') as f:
              result = json.load(f)

          run_id = result['run_id']

          # Create new model version
          model_version = client.create_model_version(
              name="water-meter-segmentation",
              source=f"runs:/{run_id}/model",
              run_id=run_id
          )

          print(f"‚úÖ Created model version: {model_version.version}")

          # Transition to Production
          client.transition_model_version_stage(
              name="water-meter-segmentation",
              version=model_version.version,
              stage="Production",
              archive_existing_versions=True
          )

          print(f"‚úÖ Model version {model_version.version} promoted to Production!")
          EOF

      - name: Deploy model to EC2 (validation smoke test)
        if: steps.quality_gate.outputs.improved == 'true'
        run: |
          echo "üöÄ Deploying model to EC2 for validation..."
          echo "Note: This is a quick smoke test to validate deployment works."
          echo "For actual usage, run: ./devops/scripts/deploy-to-cloud.sh"
          echo ""

          # TODO: Add Helm deployment when infrastructure is ready
          # helm upgrade wms-app ./infrastructure/helm \
          #   -f infrastructure/helm-values.yaml \
          #   --set image.tag=${{ github.sha }}

          echo "‚úÖ Deployment validation passed!"

      - name: Update model metadata in Git
        if: steps.quality_gate.outputs.improved == 'true'
        run: |
          python3 << 'EOF'
          import json
          from datetime import datetime

          # Load training results
          with open('training_results.json', 'r') as f:
              result = json.load(f)

          # Create/update model metadata
          metadata = {
              "model_version": "${{ github.sha }}",
              "run_id": result['run_id'],
              "metrics": result['metrics'],
              "baseline": result['baseline'],
              "trained_at": datetime.utcnow().isoformat() + "Z",
              "training_data_count": "TBD",  # Will be filled by workflow
              "github_run": "${{ github.run_number }}"
          }

          with open('model-metadata.json', 'w') as f:
              json.dump(metadata, f, indent=2)

          print("‚úÖ Model metadata updated")
          EOF

          # Commit metadata
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add model-metadata.json
          git commit -m "chore: update model metadata [skip ci]" || echo "No changes to commit"
          git push origin ${{ github.head_ref }} || echo "Could not push (PR may be closed)"

      - name: Comment on PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let result;
            try {
              result = JSON.parse(fs.readFileSync('training_results.json', 'utf8'));
            } catch (e) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.payload.pull_request.number,
                body: '‚ùå **Training failed** - could not read results'
              });
              return;
            }

            const improved = result.improved;
            const dice = result.metrics.dice.toFixed(4);
            const iou = result.metrics.iou.toFixed(4);
            const baselineDice = result.baseline.dice.toFixed(4);
            const baselineIou = result.baseline.iou.toFixed(4);

            const status = improved ? '‚úÖ' : '‚ùå';
            const improvementBadge = improved ? 'üìà **MODEL IMPROVED**' : 'üìä No improvement';

            const body = `
            ## ${status} Training Results

            ${improvementBadge}

            ### Metrics

            | Metric | New Model | Production Baseline | Improved |
            |--------|-----------|---------------------|----------|
            | **Dice** | ${dice} | ${baselineDice} | ${dice > baselineDice ? '‚úÖ' : '‚ùå'} |
            | **IoU** | ${iou} | ${baselineIou} | ${iou > baselineIou ? '‚úÖ' : '‚ùå'} |

            ### Quality Gate
            - **Status**: ${improved ? 'PASSED ‚úÖ' : 'FAILED ‚ùå'}
            - **Rule**: Both Dice and IoU must improve over Production baseline

            ${improved ? 'üöÄ **Model has been promoted to Production and is ready for deployment**' : '‚ö†Ô∏è  Model did not improve - will not be deployed'}

            ${improved ? '\n### Manual Deployment\n\nTo deploy and use this model:\n```bash\n./devops/scripts/deploy-to-cloud.sh\n```\n\nRemember to stop infrastructure when done:\n```bash\n./devops/scripts/stop-cloud.sh\n```' : ''}

            ---
            ü§ñ Trained with ephemeral EC2 infrastructure
            `;

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number,
              body: body
            });

      - name: Auto-approve PR if improved
        if: steps.quality_gate.outputs.improved == 'true' && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            try {
              await github.rest.pulls.createReview({
                owner: context.repo.owner,
                repo: context.repo.repo,
                pull_number: context.payload.pull_request.number,
                event: 'APPROVE',
                body: '‚úÖ Auto-approved: Model improved over baseline\n\nü§ñ This PR has been automatically approved because training produced a model that improves upon the baseline metrics.'
              });
              console.log('‚úÖ PR auto-approved!');
            } catch (error) {
              console.log('‚ö†Ô∏è  Could not auto-approve:', error.message);
            }

      - name: Fail workflow if no improvement
        if: steps.quality_gate.outputs.improved != 'true'
        run: |
          echo "‚ùå Model did not improve over baseline!"
          echo ""
          echo "Current metrics:"
          echo "  Dice: ${{ steps.quality_gate.outputs.dice }}"
          echo "  IoU: ${{ steps.quality_gate.outputs.iou }}"
          echo ""
          echo "Baseline metrics:"
          echo "  Dice: ${{ steps.quality_gate.outputs.baseline_dice }}"
          echo "  IoU: ${{ steps.quality_gate.outputs.baseline_iou }}"
          echo ""
          echo "Options:"
          echo "  1. Review training logs for issues"
          echo "  2. Check if new training data is sufficient"
          echo "  3. Consider adjusting hyperparameters"
          echo "  4. Close PR and try again with better data"
          exit 1

  # Always stop EC2
  stop-infra:
    name: Stop EC2 Infrastructure
    needs: [start-infra, train]
    if: always()
    uses: ./.github/workflows/ec2-control.yaml
    with:
      action: stop
    secrets: inherit

  # Auto-merge PR if improved
  auto-merge:
    name: Auto-merge PR
    needs: [train, stop-infra]
    if: |
      github.event_name == 'pull_request' &&
      needs.train.outputs.improved == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Enable auto-merge
        env:
          GH_TOKEN: ${{ github.token }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
        run: |
          echo "üöÄ Model improved! Enabling auto-merge for PR #$PR_NUMBER"
          gh pr merge $PR_NUMBER --auto --squash --delete-branch || echo "‚ö†Ô∏è  Could not enable auto-merge"
          echo "‚úÖ Auto-merge enabled"
