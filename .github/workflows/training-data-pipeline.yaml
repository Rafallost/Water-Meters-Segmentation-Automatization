name: Training Data Pipeline

# Complete pipeline for training data changes:
# 1. Download existing dataset from S3
# 2. Merge existing + new data
# 3. Data QA validation on merged dataset
# 4. Update DVC tracking
# 5. Create/Update Pull Request for review and training
#
# Note: Data merging moved to GitHub Actions (no local AWS credentials needed)

on:
  push:
    branches:
      - 'data/**'

permissions:
  contents: write
  pull-requests: write

jobs:
  merge-and-validate:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout data branch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}
          fetch-depth: 0
          submodules: recursive

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          # Use pip legacy resolver to avoid resolution-too-deep errors with DVC dependencies
          pip install --use-deprecated=legacy-resolver \
            'dvc[s3]>=3.50.0,<4.0.0' \
            pyyaml pillow \
            'boto3>=1.34.0,<2.0.0' \
            opencv-python numpy

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Count new files
        id: count_new
        run: |
          NEW_IMAGES=$(find WMS/data/training/images -type f \( -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" \) 2>/dev/null | wc -l)
          NEW_MASKS=$(find WMS/data/training/masks -type f \( -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" \) 2>/dev/null | wc -l)
          echo "new_images=$NEW_IMAGES" >> $GITHUB_OUTPUT
          echo "new_masks=$NEW_MASKS" >> $GITHUB_OUTPUT
          echo "üìä New files in branch: $NEW_IMAGES images, $NEW_MASKS masks"

      - name: Download existing dataset from S3
        id: download
        run: |
          echo "üì• Downloading existing dataset from S3..."

          # Check if .dvc files exist (not first upload)
          if [ -f "WMS/data/training/images.dvc" ]; then
            # Create temp directory for existing data
            mkdir -p /tmp/existing_data/images /tmp/existing_data/masks

            # Pull existing data
            dvc pull WMS/data/training/images.dvc WMS/data/training/masks.dvc || {
              echo "‚ö†Ô∏è  DVC pull failed - assuming first upload"
              echo "existing_images=0" >> $GITHUB_OUTPUT
              echo "existing_masks=0" >> $GITHUB_OUTPUT
              exit 0
            }

            # Move existing data to temp (if directories exist and have files)
            if [ -d "WMS/data/training/images" ] && [ "$(ls -A WMS/data/training/images 2>/dev/null)" ]; then
              mv WMS/data/training/images/* /tmp/existing_data/images/ 2>/dev/null || true
            fi
            if [ -d "WMS/data/training/masks" ] && [ "$(ls -A WMS/data/training/masks 2>/dev/null)" ]; then
              mv WMS/data/training/masks/* /tmp/existing_data/masks/ 2>/dev/null || true
            fi

            # Count existing files
            EXISTING_IMAGES=$(find /tmp/existing_data/images -type f \( -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" \) 2>/dev/null | wc -l)
            EXISTING_MASKS=$(find /tmp/existing_data/masks -type f \( -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" \) 2>/dev/null | wc -l)
            echo "existing_images=$EXISTING_IMAGES" >> $GITHUB_OUTPUT
            echo "existing_masks=$EXISTING_MASKS" >> $GITHUB_OUTPUT
            echo "‚úÖ Downloaded $EXISTING_IMAGES existing images from S3"
          else
            echo "‚ÑπÔ∏è  No .dvc files found - first upload"
            echo "existing_images=0" >> $GITHUB_OUTPUT
            echo "existing_masks=0" >> $GITHUB_OUTPUT
          fi

      - name: Merge datasets
        id: merge
        run: |
          echo "üì¶ Merging datasets..."

          EXISTING=${{ steps.download.outputs.existing_images }}
          NEW=${{ steps.count_new.outputs.new_images }}

          # Ensure directories exist
          mkdir -p WMS/data/training/images WMS/data/training/masks

          # Merge: copy existing data back if it exists
          if [ -d "/tmp/existing_data/images" ] && [ "$(ls -A /tmp/existing_data/images 2>/dev/null)" ]; then
            cp -n /tmp/existing_data/images/* WMS/data/training/images/ 2>/dev/null || true
          fi
          if [ -d "/tmp/existing_data/masks" ] && [ "$(ls -A /tmp/existing_data/masks 2>/dev/null)" ]; then
            cp -n /tmp/existing_data/masks/* WMS/data/training/masks/ 2>/dev/null || true
          fi

          # Count total after merge
          TOTAL_IMAGES=$(find WMS/data/training/images -type f \( -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" \) 2>/dev/null | wc -l)
          TOTAL_MASKS=$(find WMS/data/training/masks -type f \( -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" \) 2>/dev/null | wc -l)

          echo "  ‚Ä¢ Existing images: $EXISTING"
          echo "  ‚Ä¢ New images: $NEW"
          echo "  ‚Ä¢ Total: $TOTAL_IMAGES images"
          echo ""

          echo "total_images=$TOTAL_IMAGES" >> $GITHUB_OUTPUT
          echo "total_masks=$TOTAL_MASKS" >> $GITHUB_OUTPUT

      - name: Data Quality Validation
        id: qa
        run: |
          echo "üîç Validating merged dataset..."
          python devops/scripts/data-qa.py WMS/data/training/ --output report.json

          if [ $? -eq 0 ]; then
            echo "‚úÖ Data validation passed"
            echo "validation_passed=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Data validation failed"
            echo "validation_passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Update DVC tracking
        if: steps.qa.outputs.validation_passed == 'true'
        run: |
          set -e  # Exit on any error

          echo "üìã Updating DVC tracking..."

          # Configure git
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Step 1: FORCE remove from Git tracking (even if already committed)
          echo "üóëÔ∏è  Removing training data from Git tracking..."
          git rm -r --cached WMS/data/training/images WMS/data/training/masks 2>/dev/null || true

          # Commit removal if there are staged changes
          if ! git diff --staged --quiet 2>/dev/null; then
            git commit -m "chore: stop tracking training data in Git (moving to DVC) [skip ci]"
            git push origin ${{ github.ref_name }}
            echo "‚úÖ Removed from Git tracking and committed"
          else
            echo "‚ÑπÔ∏è  Files not tracked by Git or already removed"
          fi

          # Step 2: Ensure files exist on disk (they should, but verify)
          if [ ! -d "WMS/data/training/images" ] || [ -z "$(ls -A WMS/data/training/images 2>/dev/null)" ]; then
            echo "‚ùå ERROR: Training images directory is empty!"
            exit 1
          fi

          # Step 3: Add to DVC
          echo "üì¶ Adding data to DVC..."
          dvc add WMS/data/training/images || {
            echo "‚ùå ERROR: dvc add images failed"
            echo "Debug info:"
            ls -la WMS/data/training/images/ | head -5
            exit 1
          }

          dvc add WMS/data/training/masks || {
            echo "‚ùå ERROR: dvc add masks failed"
            echo "Debug info:"
            ls -la WMS/data/training/masks/ | head -5
            exit 1
          }

          echo "‚úÖ Data added to DVC successfully"

          # Step 4: Push to S3 (may fail due to AWS Academy restrictions)
          echo "üì§ Pushing data to S3..."
          if dvc push; then
            echo "‚úÖ Data pushed to S3"
          else
            echo "‚ö†Ô∏è  DVC push failed (likely AWS Academy restrictions)"
            echo "Continuing anyway - .dvc files will be committed"
          fi

          # Step 5: Verify .dvc files were created
          if [ ! -f "WMS/data/training/images.dvc" ]; then
            echo "‚ùå ERROR: images.dvc file was not created!"
            exit 1
          fi

          if [ ! -f "WMS/data/training/masks.dvc" ]; then
            echo "‚ùå ERROR: masks.dvc file was not created!"
            exit 1
          fi

          echo "‚úÖ .dvc files verified"

          # Step 6: Commit .dvc files ONLY (not .gitignore - it blocks future commits)
          echo "üíæ Committing DVC metadata..."
          git add WMS/data/training/images.dvc WMS/data/training/masks.dvc

          # Note: .gitignore is created by DVC but we DON'T commit it to avoid blocking future data uploads

          if ! git diff --staged --quiet; then
            TOTAL_IMG="${{ steps.merge.outputs.total_images }}"
            EXISTING_IMG="${{ steps.download.outputs.existing_images }}"
            NEW_IMG="${{ steps.count_new.outputs.new_images }}"

            git commit -m "data: add DVC metadata for training dataset" \
                       -m "" \
                       -m "Total images: ${TOTAL_IMG}" \
                       -m "- Existing from S3: ${EXISTING_IMG}" \
                       -m "- New in this PR: ${NEW_IMG}" \
                       -m "" \
                       -m "Files tracked by DVC:" \
                       -m "- WMS/data/training/images.dvc" \
                       -m "- WMS/data/training/masks.dvc" \
                       -m "" \
                       -m "[skip ci]"

            git push origin ${{ github.ref_name }}
            echo "‚úÖ DVC metadata committed and pushed"

            # Verify push succeeded
            git log --oneline -1
          else
            echo "‚ùå ERROR: No .dvc files to commit!"
            echo "Debug: checking what was staged..."
            git status
            exit 1
          fi

          # Step 7: Final verification
          echo ""
          echo "=== Final State Verification ==="
          echo "Files tracked by Git:"
          git ls-files WMS/data/training/ | head -10
          echo ""
          echo "Files on disk:"
          ls -la WMS/data/training/images/ | head -5
          echo ""
          echo "‚úÖ DVC tracking update complete!"

      - name: Create Pull Request
        if: steps.qa.outputs.validation_passed == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Check if PR already exists
          EXISTING_PR=$(gh pr list --head ${{ github.ref_name }} --base main --json number --jq '.[0].number' || echo "")

          if [ -n "$EXISTING_PR" ]; then
            echo "‚ÑπÔ∏è  PR #$EXISTING_PR already exists for branch ${{ github.ref_name }}"
            echo "pr_number=$EXISTING_PR" >> $GITHUB_OUTPUT
          else
            echo "üìù Creating new Pull Request..."

            # Create PR body
            PR_BODY=$(cat <<'EOF'
          ## üìä Training Data Update

          **Merged Dataset Summary:**
          - Existing images from S3: ${{ steps.download.outputs.existing_images }}
          - New images in this PR: ${{ steps.count_new.outputs.new_images }}
          - **Total dataset size: ${{ steps.merge.outputs.total_images }} images**

          ## ‚úÖ Data Quality Validation

          - [x] Image/mask pairs validated
          - [x] Image resolutions checked (512x512)
          - [x] Binary masks validated (0/255)
          - [x] File format checks passed

          ## üöÄ Next Steps

          The training pipeline will:
          1. Train U-Net model on merged dataset (${{ steps.merge.outputs.total_images }} images)
          2. Compare metrics (Dice, IoU) against Production baseline
          3. If improved: promote to Production and auto-merge
          4. If not improved: block merge with detailed metrics

          ## üìã Quality Gate Rules

          For this PR to be merged, the trained model must:
          - Dice coefficient ‚â• Production baseline (dynamic)
          - IoU ‚â• Production baseline (dynamic)

          ---
          ü§ñ *Merged by GitHub Actions - data pipeline*
          EOF
          )

            # Create PR using gh CLI
            PR_URL=$(gh pr create \
              --title "data: add ${{ steps.count_new.outputs.new_images }} new training images (total: ${{ steps.merge.outputs.total_images }})" \
              --body "$PR_BODY" \
              --base main \
              --head ${{ github.ref_name }})

            PR_NUMBER=$(echo "$PR_URL" | grep -oP '(?<=pull/)\d+')
            echo "‚úÖ Created PR #$PR_NUMBER: $PR_URL"
            echo "pr_number=$PR_NUMBER" >> $GITHUB_OUTPUT
          fi

      - name: Comment on commit (validation failed)
        if: steps.qa.outputs.validation_passed == 'false'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let report;
            try {
              report = JSON.parse(fs.readFileSync('report.json', 'utf8'));
            } catch (e) {
              report = { errors: ['Could not read QA report'] };
            }

            let errorList = '';
            if (report.errors && report.errors.length > 0) {
              errorList = report.errors.slice(0, 10).map(e => `- ${e}`).join('\n');
              if (report.errors.length > 10) {
                errorList += `\n- ... and ${report.errors.length - 10} more errors`;
              }
            }

            const errorMessage = `## ‚ùå Data Validation Failed

            The training data validation failed. Please fix the following issues and push again.

            ### Errors

            ${errorList || 'Unknown error - check workflow logs'}

            ### Statistics

            - **Images**: ${report.image_count || 0}
            - **Masks**: ${report.mask_count || 0}
            - **Valid pairs**: ${report.valid_pairs || 0}

            ### How to Fix

            1. Review the errors above
            2. Fix the issues in your local repository
            3. Commit and push to the same branch (\`${context.ref.replace('refs/heads/', '')}\`)
            4. This workflow will run again automatically

            ---
            ü§ñ Generated by [GitHub Actions](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})
            `;

            await github.rest.repos.createCommitComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              commit_sha: context.sha,
              body: errorMessage
            });

      - name: Workflow Summary
        if: always()
        run: |
          echo "## Training Data Pipeline" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.qa.outputs.validation_passed }}" == "true" ]; then
            echo "‚úÖ **Status**: PASSED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Dataset Summary:**" >> $GITHUB_STEP_SUMMARY
            echo "- Existing images: ${{ steps.download.outputs.existing_images }}" >> $GITHUB_STEP_SUMMARY
            echo "- New images: ${{ steps.count_new.outputs.new_images }}" >> $GITHUB_STEP_SUMMARY
            echo "- Total: ${{ steps.merge.outputs.total_images }} images" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Training data validated and PR created." >> $GITHUB_STEP_SUMMARY
            echo "Training pipeline will run automatically." >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Status**: FAILED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Data validation failed. See errors in commit comment." >> $GITHUB_STEP_SUMMARY
          fi
