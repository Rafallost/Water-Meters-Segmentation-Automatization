name: Training Data Pipeline

# Complete pipeline for training data changes:
# 1. Download existing dataset from S3
# 2. Merge existing + new data
# 3. Data QA validation on merged dataset
# 4. Update DVC tracking
# 5. Create/Update Pull Request for review and training
#
# Note: Data merging moved to GitHub Actions (no local AWS credentials needed)

on:
  push:
    branches:
      - 'data/**'

permissions:
  contents: write
  pull-requests: write

jobs:
  merge-and-validate:
    runs-on: ubuntu-latest
    outputs:
      validation_passed: ${{ steps.qa.outputs.validation_passed }}
      total_images: ${{ steps.merge.outputs.total_images }}
      new_images: ${{ steps.count_new.outputs.new_images }}
      existing_images: ${{ steps.download.outputs.existing_images }}

    steps:
      - name: Checkout data branch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}
          fetch-depth: 0
          submodules: recursive

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          # Use pip legacy resolver to avoid resolution-too-deep errors with DVC dependencies
          pip install --use-deprecated=legacy-resolver \
            'dvc[s3]>=3.50.0,<4.0.0' \
            pyyaml pillow \
            'boto3>=1.34.0,<2.0.0' \
            opencv-python numpy

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Count new files
        id: count_new
        run: |
          NEW_IMAGES=$(find WMS/data/training/images -type f \( -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" \) 2>/dev/null | wc -l)
          NEW_MASKS=$(find WMS/data/training/masks -type f \( -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" \) 2>/dev/null | wc -l)
          echo "new_images=$NEW_IMAGES" >> $GITHUB_OUTPUT
          echo "new_masks=$NEW_MASKS" >> $GITHUB_OUTPUT
          echo "üìä New files in branch: $NEW_IMAGES images, $NEW_MASKS masks"

      - name: Download existing dataset from S3
        id: download
        run: |
          echo "üì• Downloading existing dataset from S3..."

          # Check if .dvc files exist (not first upload)
          if [ -f "WMS/data/training/images.dvc" ]; then
            # Create temp directory for existing data
            mkdir -p /tmp/existing_data/images /tmp/existing_data/masks

            # Pull existing data
            dvc pull WMS/data/training/images.dvc WMS/data/training/masks.dvc || {
              echo "‚ö†Ô∏è  DVC pull failed - assuming first upload"
              echo "existing_images=0" >> $GITHUB_OUTPUT
              echo "existing_masks=0" >> $GITHUB_OUTPUT
              exit 0
            }

            # Move existing data to temp (if directories exist and have files)
            if [ -d "WMS/data/training/images" ] && [ "$(ls -A WMS/data/training/images 2>/dev/null)" ]; then
              mv WMS/data/training/images/* /tmp/existing_data/images/ 2>/dev/null || true
            fi
            if [ -d "WMS/data/training/masks" ] && [ "$(ls -A WMS/data/training/masks 2>/dev/null)" ]; then
              mv WMS/data/training/masks/* /tmp/existing_data/masks/ 2>/dev/null || true
            fi

            # Count existing files
            EXISTING_IMAGES=$(find /tmp/existing_data/images -type f \( -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" \) 2>/dev/null | wc -l)
            EXISTING_MASKS=$(find /tmp/existing_data/masks -type f \( -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" \) 2>/dev/null | wc -l)
            echo "existing_images=$EXISTING_IMAGES" >> $GITHUB_OUTPUT
            echo "existing_masks=$EXISTING_MASKS" >> $GITHUB_OUTPUT
            echo "‚úÖ Downloaded $EXISTING_IMAGES existing images from S3"
          else
            echo "‚ÑπÔ∏è  No .dvc files found - first upload"
            echo "existing_images=0" >> $GITHUB_OUTPUT
            echo "existing_masks=0" >> $GITHUB_OUTPUT
          fi

      - name: Merge datasets
        id: merge
        run: |
          echo "üì¶ Merging datasets..."

          EXISTING=${{ steps.download.outputs.existing_images }}
          NEW=${{ steps.count_new.outputs.new_images }}

          # Ensure directories exist
          mkdir -p WMS/data/training/images WMS/data/training/masks

          # Merge: copy existing data back if it exists
          if [ -d "/tmp/existing_data/images" ] && [ "$(ls -A /tmp/existing_data/images 2>/dev/null)" ]; then
            cp -n /tmp/existing_data/images/* WMS/data/training/images/ 2>/dev/null || true
          fi
          if [ -d "/tmp/existing_data/masks" ] && [ "$(ls -A /tmp/existing_data/masks 2>/dev/null)" ]; then
            cp -n /tmp/existing_data/masks/* WMS/data/training/masks/ 2>/dev/null || true
          fi

          # Count total after merge
          TOTAL_IMAGES=$(find WMS/data/training/images -type f \( -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" \) 2>/dev/null | wc -l)
          TOTAL_MASKS=$(find WMS/data/training/masks -type f \( -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" \) 2>/dev/null | wc -l)

          echo "  ‚Ä¢ Existing images: $EXISTING"
          echo "  ‚Ä¢ New images: $NEW"
          echo "  ‚Ä¢ Total: $TOTAL_IMAGES images"
          echo ""

          echo "total_images=$TOTAL_IMAGES" >> $GITHUB_OUTPUT
          echo "total_masks=$TOTAL_MASKS" >> $GITHUB_OUTPUT

      - name: Data Quality Validation
        id: qa
        run: |
          echo "üîç Validating merged dataset..."
          python devops/scripts/data-qa.py WMS/data/training/ --output report.json

          if [ $? -eq 0 ]; then
            echo "‚úÖ Data validation passed"
            echo "validation_passed=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Data validation failed"
            echo "validation_passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Update DVC tracking
        if: steps.qa.outputs.validation_passed == 'true'
        run: |
          set -e  # Exit on any error

          echo "üìã Updating DVC tracking..."

          # Configure git
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Step 1: FORCE remove from Git tracking (even if already committed)
          echo "üóëÔ∏è  Removing training data from Git tracking..."
          git rm -r --cached WMS/data/training/images WMS/data/training/masks 2>/dev/null || true

          # Commit removal if there are staged changes
          if ! git diff --staged --quiet 2>/dev/null; then
            git commit -m "chore: stop tracking training data in Git (moving to DVC)"
            git push origin ${{ github.ref_name }}
            echo "‚úÖ Removed from Git tracking and committed"
          else
            echo "‚ÑπÔ∏è  Files not tracked by Git or already removed"
          fi

          # Step 2: Ensure files exist on disk (they should, but verify)
          if [ ! -d "WMS/data/training/images" ] || [ -z "$(ls -A WMS/data/training/images 2>/dev/null)" ]; then
            echo "‚ùå ERROR: Training images directory is empty!"
            exit 1
          fi

          # Step 3: Add to DVC (exclude .gitkeep from tracking)
          echo "üì¶ Adding data to DVC..."

          # Remove .gitkeep temporarily (DVC shouldn't track them, only data files)
          rm -f WMS/data/training/images/.gitkeep WMS/data/training/masks/.gitkeep 2>/dev/null || true

          dvc add WMS/data/training/images || {
            echo "‚ùå ERROR: dvc add images failed"
            echo "Debug info:"
            ls -la WMS/data/training/images/ | head -5
            exit 1
          }

          dvc add WMS/data/training/masks || {
            echo "‚ùå ERROR: dvc add masks failed"
            echo "Debug info:"
            ls -la WMS/data/training/masks/ | head -5
            exit 1
          }

          # Recreate .gitkeep files (for Git to track empty dirs, but DVC ignores them)
          touch WMS/data/training/images/.gitkeep WMS/data/training/masks/.gitkeep

          echo "‚úÖ Data added to DVC successfully (nfiles excludes .gitkeep)"

          # Step 4: Push to S3 (may fail due to AWS Academy restrictions)
          echo "üì§ Pushing data to S3..."
          if dvc push; then
            echo "‚úÖ Data pushed to S3"
          else
            echo "‚ö†Ô∏è  DVC push failed (likely AWS Academy restrictions)"
            echo "Continuing anyway - .dvc files will be committed"
          fi

          # Step 5: Verify .dvc files were created
          if [ ! -f "WMS/data/training/images.dvc" ]; then
            echo "‚ùå ERROR: images.dvc file was not created!"
            exit 1
          fi

          if [ ! -f "WMS/data/training/masks.dvc" ]; then
            echo "‚ùå ERROR: masks.dvc file was not created!"
            exit 1
          fi

          echo "‚úÖ .dvc files verified"

          # Step 6: Commit .dvc files ONLY (not .gitignore - it blocks future commits)
          echo "üíæ Committing DVC metadata..."
          git add WMS/data/training/images.dvc WMS/data/training/masks.dvc

          # Note: .gitignore is created by DVC but we DON'T commit it to avoid blocking future data uploads

          if ! git diff --staged --quiet; then
            TOTAL_IMG="${{ steps.merge.outputs.total_images }}"
            EXISTING_IMG="${{ steps.download.outputs.existing_images }}"
            NEW_IMG="${{ steps.count_new.outputs.new_images }}"

            git commit -m "data: add DVC metadata for training dataset" \
                       -m "" \
                       -m "Total images: ${TOTAL_IMG}" \
                       -m "- Existing from S3: ${EXISTING_IMG}" \
                       -m "- New in this PR: ${NEW_IMG}" \
                       -m "" \
                       -m "Files tracked by DVC:" \
                       -m "- WMS/data/training/images.dvc" \
                       -m "- WMS/data/training/masks.dvc"

            git push origin ${{ github.ref_name }}
            echo "‚úÖ DVC metadata committed and pushed"

            # Verify push succeeded
            git log --oneline -1
          else
            echo "‚ÑπÔ∏è  .dvc files unchanged (data already tracked with same hash)"
            echo "   Continuing with existing DVC metadata..."
          fi

          # Step 7: Final verification
          echo ""
          echo "=== Final State Verification ==="
          echo "Files tracked by Git:"
          git ls-files WMS/data/training/ | head -10
          echo ""
          echo "Files on disk:"
          ls -la WMS/data/training/images/ | head -5
          echo ""
          echo "‚úÖ DVC tracking update complete!"

      - name: Comment on commit (validation failed)
        if: steps.qa.outputs.validation_passed == 'false'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let report;
            try {
              report = JSON.parse(fs.readFileSync('report.json', 'utf8'));
            } catch (e) {
              report = { errors: ['Could not read QA report'] };
            }

            let errorList = '';
            if (report.errors && report.errors.length > 0) {
              errorList = report.errors.slice(0, 10).map(e => `- ${e}`).join('\n');
              if (report.errors.length > 10) {
                errorList += `\n- ... and ${report.errors.length - 10} more errors`;
              }
            }

            const errorMessage = `## ‚ùå Data Validation Failed

            The training data validation failed. Please fix the following issues and push again.

            ### Errors

            ${errorList || 'Unknown error - check workflow logs'}

            ### Statistics

            - **Images**: ${report.image_count || 0}
            - **Masks**: ${report.mask_count || 0}
            - **Valid pairs**: ${report.valid_pairs || 0}

            ### How to Fix

            1. Review the errors above
            2. Fix the issues in your local repository
            3. Commit and push to the same branch (\`${context.ref.replace('refs/heads/', '')}\`)
            4. This workflow will run again automatically

            ---
            ü§ñ Generated by [GitHub Actions](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})
            `;

            await github.rest.repos.createCommitComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              commit_sha: context.sha,
              body: errorMessage
            });

      - name: Workflow Summary
        if: always()
        run: |
          echo "## Training Data Pipeline - Data Validation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.qa.outputs.validation_passed }}" == "true" ]; then
            echo "‚úÖ **Status**: PASSED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Dataset Summary:**" >> $GITHUB_STEP_SUMMARY
            echo "- Existing images: ${{ steps.download.outputs.existing_images }}" >> $GITHUB_STEP_SUMMARY
            echo "- New images: ${{ steps.count_new.outputs.new_images }}" >> $GITHUB_STEP_SUMMARY
            echo "- Total: ${{ steps.merge.outputs.total_images }} images" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Next: Training model to check if it improves..." >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Status**: FAILED" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Data validation failed. See errors in commit comment." >> $GITHUB_STEP_SUMMARY
          fi

  # Start EC2 infrastructure for training
  start-infra:
    name: Start EC2 Infrastructure
    needs: merge-and-validate
    if: needs.merge-and-validate.outputs.validation_passed == 'true'
    uses: ./.github/workflows/ec2-control.yaml
    with:
      action: start
    secrets: inherit

  # Train model and check if it improves
  train:
    name: Train Model
    needs: start-infra
    runs-on: ubuntu-latest
    timeout-minutes: 30
    outputs:
      improved: ${{ steps.quality_gate.outputs.improved }}
      dice: ${{ steps.quality_gate.outputs.dice }}
      iou: ${{ steps.quality_gate.outputs.iou }}
      run_id: ${{ steps.quality_gate.outputs.run_id }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}
          submodules: recursive

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install dvc[s3]

      - name: Pull training data with DVC (if needed)
        run: |
          cd WMS/data/training

          # Check if data already in repo (from data branch)
          if [ -d "images" ] && [ "$(ls -A images)" ]; then
            echo "‚úÖ Training data already in repository"
          elif [ -f "images.dvc" ]; then
            echo "üì• Pulling data from S3 with DVC..."
            dvc pull images.dvc masks.dvc || echo "‚ö†Ô∏è  DVC pull failed, using local data"
          else
            echo "‚ö†Ô∏è  No training data found"
            exit 1
          fi

      - name: Wait for MLflow server
        run: |
          MLFLOW_URL="${{ needs.start-infra.outputs.mlflow_url }}"
          echo "‚è≥ Waiting for MLflow server at $MLFLOW_URL"

          MAX_ATTEMPTS=30
          ATTEMPT=0

          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if curl -sf "$MLFLOW_URL/health" > /dev/null 2>&1; then
              echo "‚úÖ MLflow server is ready!"
              exit 0
            fi

            ATTEMPT=$((ATTEMPT + 1))
            echo "Attempt $ATTEMPT/$MAX_ATTEMPTS - waiting 10s..."
            sleep 10
          done

          echo "‚ùå MLflow server did not become ready (5 minutes)"
          exit 1

      - name: Train model
        env:
          MLFLOW_TRACKING_URI: ${{ needs.start-infra.outputs.mlflow_url }}
          MLFLOW_HTTP_REQUEST_TIMEOUT: 300
          MLFLOW_HTTP_REQUEST_MAX_RETRIES: 5
          PYTHONPATH: ${{ github.workspace }}
        run: |
          echo "=== Training Model ==="
          echo "MLflow URL: $MLFLOW_TRACKING_URI"
          cd WMS/src
          python train.py --config ../configs/train.yaml --seed ${{ github.run_number }}

      - name: Quality Gate - Compare with Production baseline
        id: quality_gate
        env:
          MLFLOW_TRACKING_URI: ${{ needs.start-infra.outputs.mlflow_url }}
          MLFLOW_HTTP_REQUEST_TIMEOUT: 300
          MLFLOW_HTTP_REQUEST_MAX_RETRIES: 5
        run: |
          python3 << 'EOF'
          import mlflow
          import json
          import os

          mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
          client = mlflow.tracking.MlflowClient()

          # Get latest run from experiment
          experiment = client.get_experiment_by_name("water-meter-segmentation")
          if not experiment:
              print("‚ùå Experiment not found")
              exit(1)

          runs = client.search_runs(
              experiment_ids=[experiment.experiment_id],
              order_by=["start_time DESC"],
              max_results=1
          )

          if not runs:
              print("‚ùå No runs found")
              exit(1)

          run = runs[0]
          metrics = run.data.metrics

          # Get current model metrics
          current_dice = (
              metrics.get('final_test_dice') or
              metrics.get('test_dice') or
              metrics.get('val_dice', 0)
          )
          current_iou = (
              metrics.get('final_test_iou') or
              metrics.get('test_iou') or
              metrics.get('val_iou', 0)
          )

          # Get baseline from Production model (if exists)
          try:
              prod_versions = client.get_latest_versions("water-meter-segmentation", stages=["Production"])
              if prod_versions:
                  prod_version = prod_versions[0]
                  prod_run = client.get_run(prod_version.run_id)

                  prod_metrics = prod_run.data.metrics
                  baseline_dice = (
                      prod_metrics.get('final_test_dice') or
                      prod_metrics.get('test_dice') or
                      prod_metrics.get('val_dice', 0)
                  )
                  baseline_iou = (
                      prod_metrics.get('final_test_iou') or
                      prod_metrics.get('test_iou') or
                      prod_metrics.get('val_iou', 0)
                  )
                  print(f"üìä Production baseline: Dice={baseline_dice:.4f}, IoU={baseline_iou:.4f}")
              else:
                  baseline_dice = 0.0
                  baseline_iou = 0.0
                  print("üìä No Production model - first training (baseline=0)")
          except Exception as e:
              print(f"‚ö†Ô∏è  Could not fetch Production baseline: {e}")
              baseline_dice = 0.0
              baseline_iou = 0.0

          # Quality gate: model must improve (no threshold, pure comparison)
          improved = current_dice > baseline_dice and current_iou > baseline_iou

          result = {
              "run_id": run.info.run_id,
              "metrics": {
                  "dice": current_dice,
                  "iou": current_iou
              },
              "baseline": {
                  "dice": baseline_dice,
                  "iou": baseline_iou
              },
              "improved": improved
          }

          print("\n" + "="*50)
          print("QUALITY GATE RESULTS")
          print("="*50)
          print(json.dumps(result, indent=2))
          print("="*50)

          # Save results
          with open('training_results.json', 'w') as f:
              json.dump(result, f, indent=2)

          # Output for GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"improved={str(improved).lower()}\n")
              f.write(f"dice={current_dice}\n")
              f.write(f"iou={current_iou}\n")
              f.write(f"run_id={run.info.run_id}\n")
              f.write(f"baseline_dice={baseline_dice}\n")
              f.write(f"baseline_iou={baseline_iou}\n")
          EOF

      - name: Upload results artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: training-results
          path: training_results.json
          retention-days: 7

      # If improved: promote model
      - name: Promote model to Production
        if: steps.quality_gate.outputs.improved == 'true'
        env:
          MLFLOW_TRACKING_URI: ${{ needs.start-infra.outputs.mlflow_url }}
          MLFLOW_HTTP_REQUEST_TIMEOUT: 300
          MLFLOW_HTTP_REQUEST_MAX_RETRIES: 5
        run: |
          python3 << 'EOF'
          import mlflow
          from mlflow.tracking import MlflowClient
          import json
          import os

          mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
          client = MlflowClient()

          # Load results
          with open('training_results.json', 'r') as f:
              result = json.load(f)

          run_id = result['run_id']

          # Promote model to Production (fail if S3 upload fails)
          model_version = client.create_model_version(
              name="water-meter-segmentation",
              source=f"runs:/{run_id}/model",
              run_id=run_id
          )

          print(f"‚úÖ Created model version: {model_version.version}")

          # Transition to Production
          client.transition_model_version_stage(
              name="water-meter-segmentation",
              version=model_version.version,
              stage="Production",
              archive_existing_versions=True
          )

          print(f"‚úÖ Model version {model_version.version} promoted to Production!")
          EOF

      - name: Training Summary
        if: always()
        run: |
          echo "## Training Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.quality_gate.outputs.improved }}" == "true" ]; then
            echo "‚úÖ **Model Improved!**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**New Metrics:**" >> $GITHUB_STEP_SUMMARY
            echo "- Dice: ${{ steps.quality_gate.outputs.dice }}" >> $GITHUB_STEP_SUMMARY
            echo "- IoU: ${{ steps.quality_gate.outputs.iou }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Baseline Metrics:**" >> $GITHUB_STEP_SUMMARY
            echo "- Dice: ${{ steps.quality_gate.outputs.baseline_dice }}" >> $GITHUB_STEP_SUMMARY
            echo "- IoU: ${{ steps.quality_gate.outputs.baseline_iou }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "PR will be created and auto-merged." >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Model Did Not Improve**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Training completed but model did not surpass baseline." >> $GITHUB_STEP_SUMMARY
            echo "No PR will be created." >> $GITHUB_STEP_SUMMARY
          fi

  # Always stop EC2
  stop-infra:
    name: Stop EC2 Infrastructure
    needs: [start-infra, train]
    if: always()
    uses: ./.github/workflows/ec2-control.yaml
    with:
      action: stop
    secrets: inherit

  # Create PR ONLY if model improved
  create-pr:
    name: Create Pull Request
    needs: [merge-and-validate, train, stop-infra]
    if: needs.train.outputs.improved == 'true'
    runs-on: ubuntu-latest
    outputs:
      pr_number: ${{ steps.create.outputs.pr_number }}

    steps:
      - name: Checkout data branch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref_name }}
          fetch-depth: 0

      - name: Create PR
        id: create
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Check if PR already exists
          EXISTING_PR=$(gh pr list --head ${{ github.ref_name }} --base main --json number --jq '.[0].number' || echo "")

          if [ -n "$EXISTING_PR" ]; then
            echo "‚ÑπÔ∏è  PR #$EXISTING_PR already exists for branch ${{ github.ref_name }}"
            echo "pr_number=$EXISTING_PR" >> $GITHUB_OUTPUT
          else
            echo "üìù Creating new Pull Request..."

            # Create PR body
            PR_BODY=$(cat <<EOF
          ## üìä Training Data Update

          **Merged Dataset Summary:**
          - Existing images from S3: ${{ needs.merge-and-validate.outputs.existing_images }}
          - New images in this PR: ${{ needs.merge-and-validate.outputs.new_images }}
          - **Total dataset size: ${{ needs.merge-and-validate.outputs.total_images }} images**

          ## ‚úÖ Data Quality Validation

          - [x] Image/mask pairs validated
          - [x] Image resolutions checked (512x512)
          - [x] Binary masks validated (0/255)
          - [x] File format checks passed

          ## üìà Training Results

          **Model Improved!**

          | Metric | New Model | Production Baseline | Improvement |
          |--------|-----------|---------------------|-------------|
          | **Dice** | ${{ needs.train.outputs.dice }} | ${{ needs.train.outputs.baseline_dice }} | ‚úÖ |
          | **IoU** | ${{ needs.train.outputs.iou }} | ${{ needs.train.outputs.baseline_iou }} | ‚úÖ |

          The model has been promoted to Production stage in MLflow.

          ---
          ü§ñ *Auto-generated by training data pipeline - model improved over baseline*
          EOF
          )

            # Create PR using gh CLI
            PR_URL=$(gh pr create \
              --title "data: add ${{ needs.merge-and-validate.outputs.new_images }} new training images (model improved)" \
              --body "$PR_BODY" \
              --base main \
              --head ${{ github.ref_name }})

            PR_NUMBER=$(echo "$PR_URL" | grep -oP '(?<=pull/)\d+')
            echo "‚úÖ Created PR #$PR_NUMBER: $PR_URL"
            echo "pr_number=$PR_NUMBER" >> $GITHUB_OUTPUT
          fi

  # Auto-merge PR if model improved
  auto-merge:
    name: Auto-merge PR
    needs: create-pr
    if: needs.create-pr.result == 'success'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Enable auto-merge
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PR_NUMBER: ${{ needs.create-pr.outputs.pr_number }}
        run: |
          echo "üöÄ Model improved! Enabling auto-merge for PR #$PR_NUMBER"
          gh pr merge $PR_NUMBER --auto --squash --delete-branch || echo "‚ö†Ô∏è  Could not enable auto-merge"
          echo "‚úÖ Auto-merge enabled"
