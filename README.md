# Water Meters Segmentation - Automated ML Pipeline

[![CI Pipeline](https://github.com/Rafallost/Water-Meters-Segmentation-Autimatization/workflows/CI%20Pipeline/badge.svg)](https://github.com/Rafallost/Water-Meters-Segmentation-Autimatization/actions)

**Automated ML training and deployment pipeline for water meter segmentation using U-Net**

> **ğŸ‘‰ Just cloned this repo? Start here:** [QUICKSTART.md](QUICKSTART.md) - Download the model first!

This project demonstrates DevOps best practices applied to machine learning, featuring:
- âœ… Automated data validation and versioning
- âœ… Ephemeral infrastructure (70% cost savings)
- âœ… Quality-gated training pipeline (3 attempts per PR)
- âœ… MLflow experiment tracking
- âœ… Infrastructure as Code (Terraform)

**Bachelor's Thesis Project:** "Application of DevOps Techniques in Implementing Automatic CI/CD Process for Training and Versioning AI Models"

---

## âš ï¸ Important: First Time Setup

**Models are NOT stored in Git!** After cloning this repository, you must download the Production model from MLflow:

```bash
# One-time setup after git clone:

# Option 1: AWS CLI (Simpler - recommended!)
python WMS/scripts/sync_model_aws.py
# Windows: just double-click sync_model_aws.bat

# Option 2: GitHub CLI (if configured)
python WMS/scripts/sync_model.py
# Windows: just double-click sync_model.bat

# Both will:
# 1. Start EC2 instance (via GitHub Actions)
# 2. Download Production model from MLflow
# 3. Save to WMS/models/production.pth (gitignored, local cache)
# 4. Stop EC2 instance

# Now you can run predictions offline!
python WMS/src/predicts.py
```

**Why not in Git?**
- âœ… Keeps repository lightweight (models are 7+ MB, can grow to GB)
- âœ… Fast git clone/pull operations (no large binary files)
- âœ… MLflow is single source of truth for model versions
- âœ… Industry-standard MLOps practice (model registry pattern)
- âœ… No Git merge conflicts with binary files

**Note:** Model is cached locally after download. You only need to re-download when a new model is trained.

---

## ğŸš€ Quick Start

### For Users: Upload New Training Data

```bash
# 1. Add your images and masks
cp /path/to/new/*.jpg WMS/data/training/images/
cp /path/to/new/*.png WMS/data/training/masks/

# 2. Commit and push (hook auto-creates branch!)
git add WMS/data/training/
git commit -m "data: add new training samples"
git push origin main  # Pre-push hook redirects to data/TIMESTAMP

# 3. Wait ~10 minutes for training
# 4. Check PR for training results
# 5. Merge if model improved!

# 6. Download the new Production model
python WMS/scripts/sync_model_aws.py --force  # Re-download latest version
# Windows: double-click sync_model_aws.bat

# 7. Use it for predictions
python WMS/src/predicts.py
```

**When to re-download model:**
- âœ… After first `git clone` (required - see First Time Setup above)
- âœ… After merging PR with improved model (optional - to get latest version)
- âŒ NOT needed after every `git pull/push` (model cached locally)
- âŒ NOT stored in Git (downloaded from MLflow on-demand)

ğŸ‘‰ **[Full model usage guide](docs/USAGE.md#-using-the-production-model-locally)**

**How it works:** Pre-push hook detects training data changes and automatically:
- Creates branch `data/YYYYMMDD-HHMMSS`
- Pushes your data there (not to main)
- Triggers automated PR â†’ validation â†’ training â†’ auto-merge flow

ğŸ‘‰ **[How hooks work](docs/BRANCH_PROTECTION.md#-two-layer-protection-system)**

**That's it!** The system handles:
- Data validation
- Model training (3 attempts with different seeds)
- Quality comparison against baseline
- Model promotion to MLflow
- Auto-approval if model improves

ğŸ‘‰ **[Full usage guide](docs/USAGE.md)**

---

## ğŸ› ï¸ Model Management Commands

Quick reference for working with ML models:

### Download Model from MLflow

```bash
# Download Production model (first time or after new training)
python WMS/scripts/sync_model_aws.py

# Options:
python WMS/scripts/sync_model_aws.py --force      # Re-download even if cached
python WMS/scripts/sync_model_aws.py --no-stop    # Keep EC2 running after download
```

**What it does:**
- âœ… Starts EC2 instance (if stopped)
- âœ… Waits for MLflow to be ready (~2-5 min)
- âœ… Downloads Production model to `WMS/models/production.pth`
- âœ… Asks if you want to stop EC2 (saves costs)

### Run Predictions

```bash
# Run predictions using downloaded model
python WMS/src/predicts.py

# Place images in: WMS/data/predictions/photos_to_predict/
# Results saved to: WMS/data/predictions/predicted_masks/
```

**Requirements:** Model must be downloaded first (see above)

### Check Model Metrics

```bash
# Quick view (no EC2 needed!)
cat WMS/models/production_current.json

# Or detailed view (requires EC2)
python WMS/scripts/show_metrics.py

# Show all model versions and their metrics
python WMS/scripts/show_metrics.py --all

# View metrics history
cat WMS/models/production_history.jsonl
```

**Output:**
- Dice score, IoU, Loss metrics
- Training parameters (LR, batch size, epochs)
- Quality assessment (Excellent/Good/Mediocre/Poor)
- All versions with timestamps
- **NEW:** Metrics tracked in Git (no EC2 needed!)

### Manage Model Versions

```bash
# Check which models exist and their stages
python WMS/scripts/check_model.py

# Promote a specific version to Production (interactive)
# Script will ask for confirmation before promoting
```

**Use cases:**
- See all model versions (1-14 in your case)
- Check which version is in Production stage
- Manually promote a model to Production

### Browse Models in MLflow UI

```bash
# 1. Start EC2 (if not running)
python WMS/scripts/sync_model_aws.py --no-stop

# 2. Open browser: http://100.49.195.150:5000
#    - Click "Models" â†’ "water-meter-segmentation"
#    - See all versions with full metrics and artifacts
#    - View training curves and model parameters
```

---

## ğŸ“š Documentation

**[â†’ Full Documentation Index](docs/README.md)** ğŸ“–

### Quick Access:

| Document | Description | When to Read |
|----------|-------------|--------------|
| **[WORKFLOWS.md](docs/WORKFLOWS.md)** | â­ **Start here!** All pipelines explained | Understanding the system |
| **[USAGE.md](docs/USAGE.md)** | Step-by-step how-to guide | Daily operations |
| **[DIAGRAMS.md](docs/DIAGRAMS.md)** | ğŸ¨ Visual system diagrams (10 Mermaid charts) | Visual learners, thesis |
| **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** | System design & components | Deep dive |
| **[MONITORING.md](docs/MONITORING.md)** | Prometheus + Grafana setup | Observability & testing |
| **[CREDENTIALS.md](docs/CREDENTIALS.md)** | ğŸ” AWS credentials management | Local development setup |
| **[BRANCH_PROTECTION.md](docs/BRANCH_PROTECTION.md)** | GitHub setup guide | One-time setup |

**For Developers:**
- [Implementation Plan](devops/PLAN.md) - Development phases
- [Terraform Docs](devops/terraform/README.md) - Infrastructure
- [Tests Guide](WMS/tests/README.md) - Unit tests
- [Scripts README](WMS/scripts/README.md) - Model management tools

**For AI Assistants:**
- [CLAUDE.md](devops/CLAUDE.md) - Project context and rules

---

## ğŸ—ï¸ System Overview

```
User â†’ Upload Data â†’ Data QA â†’ Training (EC2 auto-start)
  â†’ Quality Gate â†’ Model Promotion â†’ EC2 auto-stop
  â†’ Merge PR â†’ Deploy (future)
```

### Key Features

**Ephemeral Infrastructure**
- EC2 instance only runs during training (~10 min)
- Cost: ~$4/month instead of ~$18/month (70% savings)
- Fully automated start/stop via GitHub Actions

**Quality-Gated Training**
- 3 training attempts per PR (different random seeds)
- Compares to baseline: Dice 0.9275, IoU 0.8865
- Auto-promotes best model to MLflow Production
- Auto-approves PR if model improves

**Data Versioning**
- DVC integration with S3 backend
- Git tracks metadata, S3 stores large files
- POC mode: Small datasets tracked in Git directly

**Experiment Tracking**
- MLflow server on EC2
- Tracks metrics, hyperparameters, artifacts
- Model registry with versioning (Staging/Production)

---

## ğŸ§  Model Architecture

**U-Net for Semantic Segmentation**

```
Input: 512Ã—512 RGB image (water meter photo)
  â†“
Encoder (4 levels): 16â†’32â†’64â†’128â†’256 channels
  â†“
Bottleneck: 256 channels
  â†“
Decoder (4 levels) + skip connections
  â†“
Output: 512Ã—512 binary mask (meter region)
```

| Metric | Baseline (v1) | Current Best |
|--------|---------------|--------------|
| **Dice Coefficient** | 0.9275 | _(depends on training)_ |
| **IoU** | 0.8865 | _(depends on training)_ |
| **Parameters** | 1,965,569 | 1,965,569 |
| **Model Size** | 7.6 MB | 7.6 MB |

**Framework:** PyTorch
**Architecture:** Enhanced U-Net [(Ronneberger et al., 2015)](https://arxiv.org/abs/1505.04597)

---

## ğŸ”„ Workflows

| Workflow | Purpose | Trigger | Duration |
|----------|---------|---------|----------|
| **Train Model** | Main training pipeline | PR to main | ~10 min |
| **Data QA** | Validate data quality | PR to main | ~30 sec |
| **Data Upload** | Version data, create PR | Push to `data/*` | ~1 min |
| **CI Pipeline** | Lint and test | Every PR | ~2 min |
| **EC2 Control** | Start/stop infrastructure | Called by other workflows | ~30 sec |

ğŸ‘‰ **[Detailed workflow explanations](docs/WORKFLOWS.md)**

---

## ğŸ› ï¸ Technology Stack

| Layer | Technology |
|-------|-----------|
| **ML Framework** | PyTorch |
| **Experiment Tracking** | MLflow |
| **Data Versioning** | DVC + S3 |
| **Infrastructure** | AWS (EC2, S3, ECR) |
| **IaC** | Terraform |
| **Container Orchestration** | k3s (lightweight Kubernetes) |
| **CI/CD** | GitHub Actions |
| **Deployment** | Helm (future) |

---

## ğŸ’° Cost Breakdown

**Current (with ephemeral infrastructure):**
- EC2 (t3.large, ephemeral): ~$2-3/month
- S3 storage: ~$1/month
- **Total: ~$4/month**

**Traditional (24/7 EC2):**
- EC2 (t3.large, always on): ~$15/month
- S3 storage: ~$1/month
- **Total: ~$18/month**

**Savings: 70-80%**

ğŸ‘‰ **[Architecture details](docs/ARCHITECTURE.md)**

---

## ğŸ¯ Core Flow

```mermaid
graph TD
    A[User: Upload Data] --> B[Data QA]
    B -->|PASS| C[Create PR]
    B -->|FAIL| Z[Error Comment]
    C --> D[Start EC2]
    D --> E[Train Attempt 1]
    D --> F[Train Attempt 2]
    D --> G[Train Attempt 3]
    E --> H[Aggregate Results]
    F --> H
    G --> H
    H -->|Improved| I[Promote to Production]
    H -->|Not Improved| J[Reject PR]
    I --> K[Auto-Approve PR]
    K --> L[User: Merge PR]
    H --> M[Stop EC2]
    J --> M
```

---

## ğŸ“Š Project Status

| Phase | Status | Description |
|-------|--------|-------------|
| Phase 1: Data Foundation | âœ… Complete | DVC, data QA scripts |
| Phase 2: Core Scripts | âœ… Complete | Validation, quality gates |
| Phase 3: GitHub Workflows | âœ… Complete | All pipelines implemented |
| Phase 4: Infrastructure | âœ… Complete | Terraform, EC2, MLflow |
| Phase 5: Training Pipeline | âœ… Complete | Ephemeral training with quality gates |
| Phase 6: Deployment | ğŸš§ In Progress | Docker + k3s deployment |
| Phase 7: Documentation | âœ… Complete | Comprehensive docs |
| Phase 8: Monitoring | ğŸ“… Planned | Prometheus + Grafana |
| Phase 9: Thesis Writing | ğŸ“… Planned | Academic documentation |

---

## ğŸš¦ Getting Started

### Prerequisites

- **AWS Account** (AWS Academy Learner Lab for students)
- **GitHub Account**
- **Local Tools:**
  - Python 3.12+
  - Git
  - AWS CLI v2
  - Terraform 1.0+

### Initial Setup

```bash
# 1. Clone repository with submodules
git clone --recurse-submodules https://github.com/Rafallost/Water-Meters-Segmentation-Autimatization.git
cd Water-Meters-Segmentation-Autimatization

# 2. Configure AWS credentials
aws configure
# Or for AWS Academy: Update ~/.aws/credentials with session credentials

# 3. Deploy infrastructure
cd devops/terraform
terraform init
terraform apply

# 4. Configure GitHub Secrets
# AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN

# 5. Install pre-push hook (optional)
cp devops/hooks/pre-push .git/hooks/
chmod +x .git/hooks/pre-push
```

ğŸ‘‰ **[Detailed setup guide](docs/SETUP.md)** _(TODO)_

---

## ğŸ“ˆ Training Results

Training metrics are logged to MLflow and posted as PR comments.

**Example successful training:**

```
## âœ… Training Results (3 attempts)

ğŸ“ˆ MODEL IMPROVED

### Best Result (Attempt 2)
| Metric | Value  | Baseline | Status |
|--------|--------|----------|--------|
| Dice   | 0.9350 | 0.9275   | âœ… +0.81% |
| IoU    | 0.8920 | 0.8865   | âœ… +0.62% |

ğŸš€ Best model promoted to Production
```

Access MLflow UI: `http://<EC2_IP>:5000` (when EC2 is running)

---

## ğŸ› Troubleshooting

### Common Issues

**"Data QA failed - Non-binary mask values"**
- Masks must contain only 0 and 255
- Convert JPG masks to PNG to avoid compression artifacts
- Run `python devops/scripts/data-qa.py WMS/data/training/` locally

**"Training failed - All 3 attempts"**
- Model didn't improve over baseline
- Check if new data is sufficient/correct
- Review training logs in GitHub Actions

**"AWS credentials expired"**
- AWS Academy credentials expire every 4 hours
- Update `~/.aws/credentials` and GitHub Secrets

**"EC2 costs too high"**
- Ensure ephemeral infrastructure is working
- Run `devops/scripts/cleanup-aws.sh` when done

ğŸ‘‰ **[Full troubleshooting guide](docs/USAGE.md#-troubleshooting)**

---

## ğŸ§¹ Cleanup

**IMPORTANT:** Run this when you're done to avoid AWS costs:

```bash
cd devops
bash scripts/cleanup-aws.sh
```

This will:
- Stop EC2 instance
- Empty S3 buckets
- Destroy all Terraform-managed resources

---

## ğŸ“ Repository Structure

```
Water-Meters-Segmentation-Autimatization/
â”œâ”€â”€ WMS/                          # ML code and data
â”‚   â”œâ”€â”€ src/                      # Training, inference scripts
â”‚   â”‚   â”œâ”€â”€ train.py              # Main training script
â”‚   â”‚   â”œâ”€â”€ model.py              # U-Net architecture
â”‚   â”‚   â”œâ”€â”€ dataset.py            # Data loading
â”‚   â”‚   â””â”€â”€ prepareDataset.py     # Data splitting
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ training/             # Training data (Git/DVC)
â”‚   â”‚       â”œâ”€â”€ images/           # Input photos
â”‚   â”‚       â”œâ”€â”€ masks/            # Ground truth masks
â”‚   â”‚       â””â”€â”€ *.dvc             # DVC metadata
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ train.yaml            # Hyperparameters
â”‚   â”œâ”€â”€ models/                   # Local checkpoints (gitignored)
â”‚   â””â”€â”€ tests/                    # Unit tests
â”œâ”€â”€ .github/workflows/            # CI/CD pipelines
â”‚   â”œâ”€â”€ train.yml                 # â­ Main training workflow
â”‚   â”œâ”€â”€ data-qa.yaml              # Data validation
â”‚   â”œâ”€â”€ data-upload.yaml          # Data versioning + PR creation
â”‚   â”œâ”€â”€ ec2-control.yaml          # Infrastructure start/stop
â”‚   â”œâ”€â”€ ci.yaml                   # Linting and tests
â”‚   â””â”€â”€ data-staging.yaml         # Auto-branch creation
â”œâ”€â”€ docs/                         # ğŸ“š Documentation
â”‚   â”œâ”€â”€ WORKFLOWS.md              # All pipelines explained
â”‚   â”œâ”€â”€ USAGE.md                  # How-to guide
â”‚   â””â”€â”€ ARCHITECTURE.md           # System design
â”œâ”€â”€ devops/                       # ğŸ”§ Infrastructure (submodule)
â”‚   â”œâ”€â”€ terraform/                # IaC
â”‚   â”œâ”€â”€ helm/                     # Kubernetes deployment
â”‚   â”œâ”€â”€ scripts/                  # Automation
â”‚   â”‚   â”œâ”€â”€ data-qa.py            # Data validation
â”‚   â”‚   â”œâ”€â”€ quality-gate.py       # Model comparison
â”‚   â”‚   â””â”€â”€ cleanup-aws.sh        # Resource teardown
â”‚   â”œâ”€â”€ hooks/                    # Git hooks
â”‚   â”œâ”€â”€ PLAN.md                   # Implementation phases
â”‚   â””â”€â”€ CLAUDE.md                 # AI assistant context
â”œâ”€â”€ README.md                     # This file
â””â”€â”€ requirements.txt              # Python dependencies
```

---

## ğŸ“ Academic Context

**Bachelor's Thesis Project**
**Title:** "Application of DevOps Techniques in Implementing Automatic CI/CD Process for Training and Versioning AI Models"

**Objectives:**
1. Compare manual vs. automated ML deployment workflows
2. Demonstrate cost optimization through ephemeral infrastructure
3. Implement quality gates for model versioning
4. Document best practices for ML DevOps

**Key Results:**
- 70% cost reduction through ephemeral EC2 usage
- Automated quality-gated training pipeline
- Comprehensive experiment tracking with MLflow
- Reproducible infrastructure via Terraform

---

## ğŸ¤ Contributing

This is a thesis project, but feedback is welcome!

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is for academic purposes.

---

## ğŸ™ Acknowledgments

- Original U-Net paper: [Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597)
- Course: Fundamentals of Artificial Intelligence
- MLflow, DVC, and Terraform communities

---

## ğŸ“§ Contact

- **GitHub Issues:** [Report bugs or ask questions](https://github.com/Rafallost/Water-Meters-Segmentation-Autimatization/issues)
- **Email:** _(Your email if public)_

---

## ğŸ”— Related Repositories

- **This repo:** [Water-Meters-Segmentation-Autimatization](https://github.com/Rafallost/Water-Meters-Segmentation-Autimatization) (ML code + workflows)
- **Infrastructure repo:** [DevOps-AI-Model-Automatization](https://github.com/Rafallost/DevOps-AI-Model-Automatization) (Terraform, Helm, scripts)
- **Original model repo:** [Water-Meters-Segmentation](https://github.com/Rafallost/Water-Meters-Segmentation) (Baseline, read-only)

---

**â­ If this project helps you, please star it!**
