# System Architecture

This document explains the overall system design and components.

---

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        GITHUB REPOSITORY                        â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ ML Code      â”‚  â”‚ Workflows    â”‚  â”‚ Training     â”‚         â”‚
â”‚  â”‚ (WMS/)       â”‚  â”‚ (.github/)   â”‚  â”‚ Data (DVC)   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                â”‚                â”‚
             â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GITHUB ACTIONS (Orchestration)                    â”‚
â”‚  â€¢ Data validation                                             â”‚
â”‚  â€¢ Training coordination                                       â”‚
â”‚  â€¢ Quality gates                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AWS CLOUD                               â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  EC2 Instance (Ephemeral - starts/stops automatically)   â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚  â”‚
â”‚  â”‚  â”‚ MLflow   â”‚    â”‚ k3s      â”‚    â”‚ Model    â”‚          â”‚  â”‚
â”‚  â”‚  â”‚ Server   â”‚    â”‚ (Future) â”‚    â”‚ Serving  â”‚          â”‚  â”‚
â”‚  â”‚  â”‚ :5000    â”‚    â”‚          â”‚    â”‚ (Future) â”‚          â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ S3 Bucket   â”‚    â”‚ S3 Bucket   â”‚    â”‚ ECR          â”‚      â”‚
â”‚  â”‚ (DVC Data)  â”‚    â”‚ (MLflow)    â”‚    â”‚ (Docker)     â”‚      â”‚
â”‚  â”‚             â”‚    â”‚ (Artifacts) â”‚    â”‚ (Future)     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Components

### 1. **Version Control (GitHub)**

#### Repository Structure
```
Water-Meters-Segmentation-Autimatization/
â”œâ”€â”€ WMS/                          # ML code
â”‚   â”œâ”€â”€ src/                      # Training, inference scripts
â”‚   â”œâ”€â”€ data/training/            # Training data (Git/DVC)
â”‚   â”œâ”€â”€ configs/                  # Training configs
â”‚   â””â”€â”€ models/                   # Local model checkpoints (gitignored)
â”œâ”€â”€ .github/workflows/            # CI/CD pipelines
â”œâ”€â”€ docs/                         # Documentation
â”œâ”€â”€ devops/                       # Submodule: infrastructure code
â”‚   â”œâ”€â”€ terraform/                # Infrastructure as Code
â”‚   â”œâ”€â”€ helm/                     # Kubernetes deployment
â”‚   â”œâ”€â”€ scripts/                  # Automation scripts
â”‚   â””â”€â”€ hooks/                    # Git hooks
â””â”€â”€ README.md
```

#### Git Submodule
- **devops/** is a Git submodule pointing to `DevOps-AI-Model-Automatization` repo
- Contains infrastructure code shared across projects
- Update with: `git submodule update --remote devops`

---

### 2. **Data Management**

#### DVC (Data Version Control)
- **Purpose:** Track large files (images, masks) without bloating Git
- **Backend:** S3 bucket (`s3://wms-dvc-data-<account>/`)
- **Metadata:** `.dvc` files committed to Git

```
WMS/data/training/images.dvc   â† Git tracks this (tiny JSON)
WMS/data/training/images/       â† Actual data in S3 (large)
```

---

### 3. **ML Experiment Tracking (MLflow)**

#### Deployment
- **Runs on:** EC2 instance
- **Port:** 5000
- **Backend Store:** SQLite (file-based)
- **Artifact Store:** S3 bucket (`s3://wms-mlflow-artifacts-<account>/`)

#### What MLflow Tracks
- Training runs with hyperparameters
- Metrics: Dice, IoU, accuracy, loss
- Model artifacts (.pth files)
- Model versions and stages (Staging, Production)

#### Model Versioning
```
Model Registry:
  water-meter-segmentation
  â”œâ”€â”€ v1 (Baseline) â†’ Production
  â”‚   â””â”€â”€ Dice: 0.9275, IoU: 0.8865
  â”œâ”€â”€ v2 (Attempt 1) â†’ Staging
  â”‚   â””â”€â”€ Dice: 0.9310, IoU: 0.8890
  â””â”€â”€ v3 (Best) â†’ Production  âœ…
      â””â”€â”€ Dice: 0.9350, IoU: 0.8920
```

**Source of truth:** MLflow, not Git. Git only stores code.

---

### 4. **Infrastructure (AWS)**

#### Terraform-Managed Resources
```
VPC (10.0.0.0/16)
â”œâ”€â”€ Public Subnet (10.0.1.0/24)
â”œâ”€â”€ Internet Gateway
â””â”€â”€ Security Group
    â”œâ”€â”€ Port 22: SSH (your IP)
    â”œâ”€â”€ Port 5000: MLflow (GitHub Actions)
    â””â”€â”€ Port 8000: HTTP (future model API)

EC2 Instance (t3.large)
â”œâ”€â”€ OS: Amazon Linux 2023
â”œâ”€â”€ Storage: 100GB
â”œâ”€â”€ MLflow Server (systemd service)
â””â”€â”€ k3s (lightweight Kubernetes, future)

S3 Buckets
â”œâ”€â”€ wms-dvc-data-<account>
â””â”€â”€ wms-mlflow-artifacts-<account>

ECR Repository (future)
â””â”€â”€ wms-model (Docker images)

Elastic IP (optional, not currently used)
```

#### Ephemeral Infrastructure
- **Traditional:** EC2 runs 24/7 (~$18/month)
- **Ephemeral:** EC2 starts only during training (~$4/month)
- **How:** `ec2-control.yaml` workflow starts/stops instance
- **Cost savings:** 70-80% reduction

---

### 5. **Training Pipeline**

#### Training Runners
- **Where:** Self-hosted runner na EC2
- **Duration:** ~1-2 godziny (50 epok, t3.large CPU)
- **Attempts:** Jedna prÃ³ba (single training run)

#### Training Process
```
1. prepareDataset.py
   â†“ Splits data into train/val/test (80/10/10)
2. train.py  (single run, seed = github.run_number)
   â†“ Trains U-Net model (50 epok, early stopping patience=5)
   â†“ Logs to MLflow (metrics, artifacts)
3. Quality gate (inline, po treningu)
   â†“ Fetches dynamic baseline from MLflow Production
   â†“ Promotes to Production if improved
   â†“ dvc push (tylko jeÅ›li improved) â†’ S3 DVC bucket
```

#### Model Architecture
- **Type:** U-Net for semantic segmentation
- **Input:** 512Ã—512 RGB images (water meter photos)
- **Output:** 512Ã—512 binary masks (meter region)
- **Framework:** PyTorch
- **Size:** ~7.6MB (.pth file)

---

### 6. **CI/CD Orchestration (GitHub Actions)**

#### Workflow Orchestration
```
User Action â†’ Workflow Trigger â†’ Jobs â†’ Steps â†’ Tools

Example:
  git add WMS/data/training/images/new.jpg
  git commit -m "data: new sample"
  git push origin main
    â†“
  Pre-push hook intercepts
    â†“ Creates data/20260213-HHMMSS branch, pushes there
  training-data-pipeline.yaml (triggered on data/* push)
    â†“
  merge-and-validate â†’ start-infra â†’ train â†’ stop-infra
    â†“ if improved:
  deploy â†’ stop-after-deploy â†’ create-pr â†’ auto-merge
```

See **WORKFLOWS.md** for detailed workflow explanations.

---

### 7. **Quality Gates**

#### Data Quality Gate (`data-qa.py`)
- Runs before training
- Checks:
  - Image â†” mask pairs match
  - Resolutions match (512Ã—512)
  - Masks are binary (0, 255 only)
  - Sufficient coverage (>100 pixels)
- **Blocks training if fails**

#### Model Quality Gate (inline in `training-data-pipeline.yaml`)
- Runs after single training run
- Baseline: dynamically fetched from MLflow Production model
  - If no Production model exists: baseline = 0.0 (first training always passes)
- Condition: `new_dice > baseline_dice AND new_iou > baseline_iou`
- **Promotes to Production if improved, dvc push do S3, PR created**
- **No PR created if model did not improve**

---

## ğŸ”„ Data Flow

### Training Data Flow
```
Local Machine
  â†“ git push origin main (with new images)
Pre-Push Hook
  â†“ Creates data/TIMESTAMP branch, pushes new files there
GitHub Actions â€” merge-and-validate job
  â†“ Downloads existing data from S3 (via DVC, main branch hashes)
  â†“ Merges existing + new data on runner
  â†“ Validates merged dataset (data-qa.py)
  â†“ upload-artifact â†’ GitHub artifact storage (NIE S3 przed treningiem)
GitHub Actions â€” train job (EC2 runner)
  â†“ download-artifact (scalony dataset)
  â†“ Train model (single run, 50 epochs)
  â†“ [if improved] dvc add + dvc push â†’ S3 DVC Bucket
MLflow on EC2
  â†“ Store metrics + model artifacts
S3 MLflow Bucket
```

### Model Deployment Flow
```
MLflow Model Registry (Production stage)
  â†“ download-model.sh (on EC2)
Docker Image Build
  â†“ build-and-push.sh â†’ push to ECR
ECR
  â†“ deploy-to-k3s.sh â†’ pull image
k3s on EC2
  â†“ Serve via HTTP API (NodePort)
Users/Applications
```

---

## ğŸ” Security & Access

### AWS Credentials
- Stored in GitHub Secrets:
  - `AWS_ACCESS_KEY_ID`
  - `AWS_SECRET_ACCESS_KEY`
  - `AWS_SESSION_TOKEN` (for AWS Academy)
- **Expire every 4 hours** in AWS Academy
- **Manual update required** for each session

### EC2 Access
- SSH: Key pair (`labsuser.pem`)
- Allowed from: Your IP only (security group)

### GitHub Actions Permissions
- `contents: read` - Read repository code
- `issues: write` - Post comments on issues
- `pull-requests: write` - Create/update PRs, post comments

---

## ğŸ’° Cost Optimization

### Current Costs (~$4/month)
- **EC2 (ephemeral):** ~$2-3/month (10 min/training Ã— ~20 trainings/month)
- **S3 Storage:** ~$1/month (DVC data + MLflow artifacts)
- **ECR Storage:** $0 (future)
- **Data Transfer:** Negligible

### Traditional Costs (~$18/month)
- EC2 24/7: ~$15/month
- S3: ~$1/month
- GitHub Actions: Free (public repo)

### Budget Protection
- **Cleanup script:** `devops/scripts/cleanup-aws.sh`
- **Runs:** `terraform destroy`, empties S3 buckets
- **Use when:** Finished testing or reaching budget limit

---

## ğŸ“Š Monitoring (opcjonalne)

Prometheus + Grafana dostÄ™pne opcjonalnie â€” wÅ‚Ä…czone przez `install_monitoring = true` w `terraform.tfvars`. DostÄ™p przez SSH tunnel (port 3000). WyÅ‚Ä…czone domyÅ›lnie (~750MB RAM).

---

## ğŸš€ Deployment Architecture

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Model API (k3s)    â”‚
                â”‚   FastAPI :8000      â”‚
                â”‚   /predict endpoint  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                    â”‚   MLflow    â”‚
                    â”‚   :5000     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Model serwowany przez FastAPI w Docker (ECR â†’ k3s NodePort). Jeden pod na single-node k3s. Deploy przez Helm chart (`devops/helm/ml-model`).

---

## ğŸ“š Related Documentation

- **WORKFLOWS.md** - All workflows explained
- **SETUP.md** - How to set up infrastructure
- **USAGE.md** - How to use the system
- **devops/PLAN.md** - Implementation phases
- **devops/CLAUDE.md** - AI assistant context
